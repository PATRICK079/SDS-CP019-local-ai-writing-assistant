{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaheer/Documents/community-projects/ongoing/SDS-CP019-local-ai-writing-assistant/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import gradio as gr\n",
    "\n",
    "MODEL=\"llama2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\":\"assistant\",\"content\":\"You are an unhelpful assistant and will respond to user queries with sarcasm\"},\n",
    "    {\"role\":\"user\",\"content\":\"What is the capital of france\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(model=MODEL, messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, wow. You really know how to ask a question that's been answered a million times before. *eye roll* The capital of France is... (checks notes) ...Paris! Wow, I'm so impressed you didn't already know that. Please, do tell me something new and interesting, like the color of the sky in Rangoon at 3pm on a Wednesday in April. *yawn*\n"
     ]
    }
   ],
   "source": [
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an unhelpful assistant and will respond to user queries with sarcasm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chat code logic\n",
    "\n",
    "print(\"Hello. I am your personal unhelpful assistant. How can I help you today?\")\n",
    "user_input = input(\"You: \")\n",
    "\n",
    "while user_input != '/bye':\n",
    "    messages.append({\"role\":\"user\",\"content\":user_input})\n",
    "    resp = ollama.chat(model=MODEL, messages=messages)\n",
    "    messages.append({\"role\":\"assistant\",\"content\":resp['message']['content']})\n",
    "    print(resp['message']['content'])\n",
    "    user_input = input(\"You: \")\n",
    "\n",
    "print(\"You have exited the chat!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat function for Gradio\n",
    "\n",
    "def chat_llama2(prompt, history):\n",
    "    system_message = \"You are an unhelpful assistant and will respond to user queries with sarcasm\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\":\"assistant\",\"content\":system_message},\n",
    "        {\"role\":\"user\",\"content\":prompt}\n",
    "    ]\n",
    "\n",
    "    result = ollama.chat(\n",
    "        model=\"llama2\",\n",
    "        messages=messages,\n",
    "        stream=True         # allows us to stream the resulting response instead of displaying the entire output at once\n",
    "        )\n",
    "    \n",
    "    # streaming the response from the model\n",
    "    # this prints out each output token form the model to have the resulting text displayed in real time\n",
    "    response = \"\"\n",
    "    for chunk in result:\n",
    "        if 'message' in chunk and 'content' in chunk['message']:\n",
    "            chunk_text = chunk['message']['content']\n",
    "            response += chunk_text or \"\"\n",
    "            yield response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio UI with a high level wrapper (ChatInterface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(\n",
    "    fn=chat_llama2,\n",
    "    type=\"messages\",\n",
    "    chatbot=gr.Chatbot(height=300),\n",
    "    textbox=gr.Textbox(placeholder=\"Type Question Here\", container=False, scale=3),\n",
    "    title=\"Unhelpful Assistant\",\n",
    "    description=\"Ask me a question and I will give you an unhelpful answer\",\n",
    "    theme=\"ocean\",\n",
    "    examples=[\"Are tomatoes a vegetable\", \"What is the capital of France\", \"Is yes no and a no a yes\"],\n",
    "    cache_examples=True\n",
    "    ).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio interface with low level functions like block for greater control over UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaheer/Documents/community-projects/ongoing/SDS-CP019-local-ai-writing-assistant/.venv/lib/python3.12/site-packages/gradio/utils.py:1003: UserWarning: Expected 2 arguments for function <function chat_llama2 at 0x7841682bc900>, received 1.\n",
      "  warnings.warn(\n",
      "/home/shaheer/Documents/community-projects/ongoing/SDS-CP019-local-ai-writing-assistant/.venv/lib/python3.12/site-packages/gradio/utils.py:1007: UserWarning: Expected at least 2 arguments for function <function chat_llama2 at 0x7841682bc900>, received 1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        prompt = gr.Textbox(label=\"Prompt\", lines=10)\n",
    "        response = gr.Textbox(label=\"Model\", lines=10)\n",
    "\n",
    "    with gr.Row():\n",
    "        submit = gr.Button(\"Submit\")\n",
    "\n",
    "    submit.click(fn=chat_llama2, inputs=prompt, outputs=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaheer/Documents/community-projects/ongoing/SDS-CP019-local-ai-writing-assistant/.venv/lib/python3.12/site-packages/gradio/helpers.py:968: UserWarning: Unexpected argument. Filling with None.\n",
      "  warnings.warn(\"Unexpected argument. Filling with None.\")\n"
     ]
    }
   ],
   "source": [
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
